{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:45:59.805868Z",
     "start_time": "2018-08-27T01:45:57.121813Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DrivenDataValidator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-860278a5b547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp_randint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdrivendata_validator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDrivenDataValidator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtpot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTPOTRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DrivenDataValidator'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import interp\n",
    "from drivendata_validator import DrivenDataValidator\n",
    "import itertools\n",
    "from tpot import TPOTRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def pre_process_train_test_data(train, test, label_var, exclude_scaling):\n",
    "    labels = np.ravel(train[label_var])\n",
    "    train = pd.get_dummies(train.drop(label_var, axis=1))\n",
    "    test = pd.get_dummies(test)\n",
    "\n",
    "    # match test set and training set columns\n",
    "    to_drop = np.setdiff1d(test.columns, train.columns)\n",
    "    to_add = np.setdiff1d(train.columns, test.columns)\n",
    "\n",
    "    test.drop(to_drop, axis=1, inplace=True)\n",
    "    test = test.assign(**{c: 0 for c in to_add})\n",
    "\n",
    "    test_indices = test.index\n",
    "    train_indices = train.index\n",
    "    train_test = pd.concat([train, test])\n",
    "    # train_test.sort_values(['year', 'weekofyear'], inplace=True)\n",
    "    train_test.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    print(\"Shapes before transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \", test.shape)\n",
    "    print(\"Train + Test : \", train_test.shape)\n",
    "\n",
    "    numeric_vals = train_test.select_dtypes(include=['int64', 'float64'])\n",
    "    numeric_vals = numeric_vals.loc[:, [x for x in list(\n",
    "        numeric_vals.columns.values) if x not in exclude_scaling]]\n",
    "    scaler = StandardScaler()\n",
    "    train_test[numeric_vals.columns] = scaler.fit_transform(numeric_vals)\n",
    "\n",
    "    train = train_test.loc[train_indices, :]\n",
    "    test = train_test.loc[test_indices, :]\n",
    "\n",
    "    train[label_var] = labels\n",
    "\n",
    "    print(\"Shapes after transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \",  test.shape)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "# DATA_DIR = 'D:\\work\\git_repos\\data_driven\\Deng\\data'\n",
    "\n",
    "# define data paths\n",
    "data_paths = {'train_x': os.path.join(DATA_DIR, 'dengue_features_train.csv'),\n",
    "              'train_y': os.path.join(DATA_DIR, 'dengue_labels_train.csv'),\n",
    "              'test_x':  os.path.join(DATA_DIR, 'dengue_features_test.csv')}\n",
    "\n",
    "# load training data\n",
    "X_train = pd.read_csv(data_paths['train_x'])\n",
    "y_train = pd.read_csv(data_paths['train_y'])\n",
    "X_train.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# load test data\n",
    "X_test = pd.read_csv(data_paths['test_x'])\n",
    "X_test.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# #### The first thing to notice is that each country's surveys have wildly different numbers of columns, so we'll plan on training separate models for each country and combining our predictions for submission at the end.\n",
    "# ### Pre-process Data\n",
    "print(\"Shapes before transformation\")\n",
    "print(\"Train : \", X_train.shape)\n",
    "print(\"Train Labels : \", y_train.shape)\n",
    "print(\"Test : \", X_test.shape)\n",
    "print(\"Columns : \", X_train.columns)\n",
    "train_data = pd.merge(X_train, y_train, on=['city', 'year', 'weekofyear'])\n",
    "train_data.index = np.arange(0, train_data.shape[0])\n",
    "X_test.index = np.arange(\n",
    "    train_data.shape[0]+1, train_data.shape[0]+X_test.shape[0]+1)\n",
    "\n",
    "# Rename Variables\n",
    "train_data = train_data.rename(columns={'precipitation_amt_mm': 'pred_precip',\n",
    "                                        'reanalysis_air_temp_k': 'pred_temp',\n",
    "                                        'reanalysis_avg_temp_k': 'pred_avg_temp',\n",
    "                                        'reanalysis_dew_point_temp_k': 'pred_dew_temp',\n",
    "                                        'reanalysis_max_air_temp_k': 'pred_max_temp',\n",
    "                                        'reanalysis_min_air_temp_k': 'pred_min_temp',\n",
    "                                        'reanalysis_precip_amt_kg_per_m2': 'pred_precip_vol',\n",
    "                                        'reanalysis_specific_humidity_g_per_kg': 'pred_spec_humidity',\n",
    "                                        'reanalysis_tdtr_k': 'pred_temp_rng',\n",
    "                                        'reanalysis_relative_humidity_percent': 'pred_rel_humidity_per',\n",
    "                                        'reanalysis_sat_precip_amt_mm': 'pred_sat_precip'\n",
    "                                        })\n",
    "train_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "X_test = X_test.rename(columns={'precipitation_amt_mm': 'pred_precip',\n",
    "                                'reanalysis_air_temp_k': 'pred_temp',\n",
    "                                'reanalysis_avg_temp_k': 'pred_avg_temp',\n",
    "                                'reanalysis_dew_point_temp_k': 'pred_dew_temp',\n",
    "                                'reanalysis_max_air_temp_k': 'pred_max_temp',\n",
    "                                'reanalysis_min_air_temp_k': 'pred_min_temp',\n",
    "                                'reanalysis_precip_amt_kg_per_m2': 'pred_precip_vol',\n",
    "                                'reanalysis_specific_humidity_g_per_kg': 'pred_spec_humidity',\n",
    "                                'reanalysis_tdtr_k': 'pred_temp_rng',\n",
    "                                'reanalysis_relative_humidity_percent': 'pred_rel_humidity_per',\n",
    "                                'reanalysis_sat_precip_amt_mm': 'pred_sat_precip'\n",
    "                                })\n",
    "X_test.interpolate(method='linear', inplace=True)\n",
    "\n",
    "kelvin_cols = ['pred_temp', 'pred_avg_temp',\n",
    "               'pred_dew_temp', 'pred_max_temp', 'pred_min_temp']\n",
    "train_data.loc[:, kelvin_cols] = train_data.loc[:, kelvin_cols].copy() - 273.15\n",
    "X_test.loc[:, kelvin_cols] = X_test.loc[:, kelvin_cols].copy() - 273.15\n",
    "print(train_data.head())\n",
    "\n",
    "pred_cols = ['pred_precip', 'pred_temp', 'pred_avg_temp',\n",
    "             'pred_dew_temp', 'pred_max_temp', 'pred_min_temp',\n",
    "             'pred_temp_rng', 'pred_precip_vol', 'pred_rel_humidity_per',\n",
    "             'pred_sat_precip', 'pred_spec_humidity']\n",
    "\n",
    "sta_cols = ['station_avg_temp_c', 'station_min_temp_c', 'station_max_temp_c',\n",
    "            'station_diur_temp_rng_c', 'station_precip_mm']\n",
    "\n",
    "veg_cols = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "\n",
    "loc_cols = ['city', 'year', 'weekofyear']\n",
    "\n",
    "# to_drop = ['pred_precip', 'pred_temp', 'pred_avg_temp',\n",
    "#           'pred_dew_temp', 'pred_max_temp', 'pred_min_temp',\n",
    "#           'pred_temp_rng', 'pred_precip_vol', 'pred_rel_humidity_per',\n",
    "#           'pred_sat_precip', 'year', 'weekofyear', 'ndvi_se', 'ndvi_nw']\n",
    "\n",
    "# to_drop = ['pred_sat_precip', 'pred_dew_temp', 'pred_temp_rng',\n",
    "#            'pred_avg_temp', 'station_avg_temp_c', 'station_diur_temp_rng_c',\n",
    "#            'ndvi_se', 'ndvi_nw', 'pred_max_temp', 'pred_min_temp', 'pred_temp']\n",
    "\n",
    "# to_drop = ['pred_sat_precip', 'pred_dew_temp', 'pred_temp_rng',\n",
    "#            'pred_avg_temp', 'station_avg_temp_c', 'station_diur_temp_rng_c',\n",
    "#            'ndvi_se', 'ndvi_nw', 'pred_max_temp', 'pred_min_temp', 'pred_temp', 'year']\n",
    "\n",
    "to_drop = ['year', 'weekofyear']\n",
    "\n",
    "train_data_drop = train_data.drop(to_drop, axis=1)\n",
    "X_test_drop = X_test.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"Preprocessing Training\")\n",
    "label_var = 'total_cases'\n",
    "exclude_scaling = []\n",
    "a_train, a_test = pre_process_train_test_data(\n",
    "    train_data_drop, X_test_drop, label_var, exclude_scaling)\n",
    "X_train = a_train.drop(label_var, axis=1)\n",
    "y_train = np.ravel(a_train[label_var])\n",
    "\n",
    "# restructure train data\n",
    "all_train_data = {'features': X_train,\n",
    "                  'labels': y_train}\n",
    "\n",
    "# restructure test data\n",
    "all_test_data = {'features': a_test}\n",
    "\n",
    "tune_params = 0\n",
    "if tune_params > 0:\n",
    "    bestParams = []\n",
    "    X = all_train_data['features'].values.astype(np.float32)\n",
    "    y = all_train_data['labels'].astype(np.int32)\n",
    "    mae_score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    #         pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=5,\n",
    "    #                                             periodic_checkpoint_folder='tpot_best_models_100',\n",
    "    #                                             n_jobs=20, random_state=42, verbosity=1, memory='auto',\n",
    "    #                                             generations=100, max_eval_time_mins=10)\n",
    "    #         pipeline_optimizer.fit(X, y)\n",
    "    #         pipeline_optimizer.export('tpot_best_model_pipeline_gen_100_mae.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring='neg_mean_absolute_error', cv=5,\n",
    "    #                            periodic_checkpoint_folder='D:\\work\\git_repos\\data_driven\\Deng\\data\\tpot_best_models_500\\',\n",
    "    #                            n_jobs=20, random_state=42, verbosity=3, memory='auto',\n",
    "    #                            generations=500, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('D:\\work\\git_repos\\data_driven\\Deng\\source\\tpot_best_model_pipeline_gen500.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                        periodic_checkpoint_folder='tpot_best_models_100_feat_13',\n",
    "    #                        n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                        generations=100, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('tpot_best_model_feature_13_gen_100.py')\n",
    "\n",
    "    #     pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                     periodic_checkpoint_folder='tpot_best_models_300_feat_12',\n",
    "    #                     n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                     generations=500, max_eval_time_mins=10)\n",
    "    #     pipeline_optimizer.fit(X, y)\n",
    "    #     pipeline_optimizer.export('tpot_best_model_feature_12_gen_300.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                periodic_checkpoint_folder='tpot_best_models_100_feat_8',\n",
    "    #                n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                generations=100, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('tpot_best_model_feature_8_gen_100.py')\n",
    "\n",
    "get_predictions = 1\n",
    "if get_predictions > 0:\n",
    "    X = all_train_data['features'].values.astype(np.float32)\n",
    "    y = all_train_data['labels'].astype(np.int16)\n",
    "    X_test = all_test_data['features'].values.astype(np.float32)\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.feature_selection import SelectFwe, SelectPercentile, f_regression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import make_pipeline, make_union\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from tpot.builtins import StackingEstimator\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_regression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import make_pipeline, make_union\n",
    "    from sklearn.svm import LinearSVR\n",
    "    from tpot.builtins import StackingEstimator\n",
    "    \n",
    "    # Score on the training set was:-17.338671832502126\n",
    "    exported_pipeline = make_pipeline(\n",
    "        SelectPercentile(score_func=f_regression, percentile=25),\n",
    "        SelectFromModel(estimator=ExtraTreesRegressor(max_features=0.15000000000000002, n_estimators=100), threshold=0.05),\n",
    "        StackingEstimator(estimator=LinearSVR(C=0.5, dual=True, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001)),\n",
    "        StackingEstimator(estimator=LinearSVR(C=0.001, dual=True, epsilon=0.0001, loss=\"epsilon_insensitive\", tol=0.001)),\n",
    "        LinearSVR(C=25.0, dual=True, epsilon=0.0001, loss=\"epsilon_insensitive\", tol=0.01)\n",
    "    )\n",
    "\n",
    "    exported_pipeline.fit(X, y)\n",
    "    results = exported_pipeline.predict(X_test)\n",
    "\n",
    "    \n",
    "    submission = pd.read_csv('../data/submission_format.csv')\n",
    "    submission.loc[:, 'total_cases'] = np.array(np.ceil(results), dtype=np.int32)\n",
    "\n",
    "    ## Submission Format\n",
    "    submission.to_csv('../data/submission_all_feat.csv', index=False)\n",
    "\n",
    "    # no parameters unless we have a read_csv kwargs file\n",
    "    v = DrivenDataValidator()\n",
    "\n",
    "    if v.is_valid('../data/submission_format.csv', '../data/submission_all_feat.csv'):\n",
    "        print(\"I am awesome.\")\n",
    "    else:\n",
    "        print(\"I am not so cool.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
