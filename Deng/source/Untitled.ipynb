{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.325728Z",
     "start_time": "2018-08-26T11:50:36.884556Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info >= (3, 5):\n",
    "    from importlib.util import spec_from_file_location\n",
    "    \n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import interp\n",
    "from drivendata_validator import DrivenDataValidator\n",
    "import itertools\n",
    "from tpot import TPOTRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.367751Z",
     "start_time": "2018-08-26T11:50:38.331885Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pre_process_train_test_data(train, test, label_var, exclude_scaling):\n",
    "    labels = np.ravel(train[label_var])\n",
    "    train = pd.get_dummies(train.drop(label_var, axis=1))\n",
    "    test = pd.get_dummies(test)\n",
    "\n",
    "    # match test set and training set columns\n",
    "    to_drop = np.setdiff1d(test.columns, train.columns)\n",
    "    to_add = np.setdiff1d(train.columns, test.columns)\n",
    "\n",
    "    test.drop(to_drop, axis=1, inplace=True)\n",
    "    test = test.assign(**{c: 0 for c in to_add})\n",
    "\n",
    "    train.fillna(0, inplace=True)\n",
    "    test.fillna(0, inplace=True)\n",
    "\n",
    "    test_indices = test.index\n",
    "    train_indices = train.index\n",
    "    train_test = pd.concat([train, test])\n",
    "    train_test.sort_values(['year', 'weekofyear'], inplace=True)\n",
    "    train_test.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    print(\"Shapes before transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \", test.shape)\n",
    "    print(\"Train + Test : \", train_test.shape)\n",
    "\n",
    "    numeric_vals = train_test.select_dtypes(include=['int64', 'float64'])\n",
    "    numeric_vals = numeric_vals.loc[:, [x for x in list(numeric_vals.columns.values) if x not in exclude_scaling]]\n",
    "    scaler = StandardScaler()\n",
    "    train_test[numeric_vals.columns] = scaler.fit_transform(numeric_vals)\n",
    "\n",
    "    train = train_test.loc[train_indices, :]\n",
    "    test = train_test.loc[test_indices, :]\n",
    "\n",
    "    train[label_var] = labels\n",
    "\n",
    "    print(\"Shapes after transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \",  test.shape)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.486629Z",
     "start_time": "2018-08-26T11:50:38.373724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before transformation\n",
      "('Train : ', (1456, 23))\n",
      "('Train Labels : ', (1456, 4))\n",
      "('Test : ', (416, 23))\n",
      "('Columns : ', Index([u'city', u'year', u'weekofyear', u'ndvi_ne', u'ndvi_nw', u'ndvi_se',\n",
      "       u'ndvi_sw', u'precipitation_amt_mm', u'reanalysis_air_temp_k',\n",
      "       u'reanalysis_avg_temp_k', u'reanalysis_dew_point_temp_k',\n",
      "       u'reanalysis_max_air_temp_k', u'reanalysis_min_air_temp_k',\n",
      "       u'reanalysis_precip_amt_kg_per_m2',\n",
      "       u'reanalysis_relative_humidity_percent',\n",
      "       u'reanalysis_sat_precip_amt_mm',\n",
      "       u'reanalysis_specific_humidity_g_per_kg', u'reanalysis_tdtr_k',\n",
      "       u'station_avg_temp_c', u'station_diur_temp_rng_c',\n",
      "       u'station_max_temp_c', u'station_min_temp_c', u'station_precip_mm'],\n",
      "      dtype='object'))\n",
      "Preprocessing Training\n",
      "Shapes before transformation\n",
      "('Train : ', (1456, 24))\n",
      "('Test : ', (416, 24))\n",
      "('Train + Test : ', (1872, 24))\n",
      "Shapes after transformation\n",
      "('Train : ', (1456, 25))\n",
      "('Test : ', (416, 24))\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data'\n",
    "\n",
    "## define data paths\n",
    "data_paths = {'train_x': os.path.join(DATA_DIR, 'dengue_features_train.csv'),\n",
    "              'train_y': os.path.join(DATA_DIR, 'dengue_labels_train.csv'),\n",
    "               'test_x':  os.path.join(DATA_DIR, 'dengue_features_test.csv')}\n",
    "\n",
    "# load training data\n",
    "X_train = pd.read_csv(data_paths['train_x'])\n",
    "y_train = pd.read_csv(data_paths['train_y'])\n",
    "X_train.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# load test data\n",
    "X_test = pd.read_csv(data_paths['test_x'])\n",
    "X_test.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# #### The first thing to notice is that each country's surveys have wildly different numbers of columns, so we'll plan on training separate models for each country and combining our predictions for submission at the end.\n",
    "# ### Pre-process Data\n",
    "print(\"Shapes before transformation\")\n",
    "print(\"Train : \", X_train.shape)\n",
    "print(\"Train Labels : \", y_train.shape)\n",
    "print(\"Test : \", X_test.shape)\n",
    "print(\"Columns : \", X_train.columns)\n",
    "train_data = pd.merge(X_train, y_train, on=['city', 'year', 'weekofyear'])\n",
    "train_data.index = np.arange(0, train_data.shape[0])\n",
    "X_test.index = np.arange(train_data.shape[0]+1, train_data.shape[0]+X_test.shape[0]+1)\n",
    "\n",
    "print(\"Preprocessing Training\")\n",
    "label_var = 'total_cases'\n",
    "exclude_scaling = ['year', 'weekofyear']\n",
    "a_train, a_test = pre_process_train_test_data(train_data, X_test, label_var, exclude_scaling)\n",
    "X_train = a_train.drop(label_var, axis=1)\n",
    "y_train = np.ravel(a_train[label_var])\n",
    "\n",
    "## restructure train data\n",
    "all_train_data = {'features': X_train,\n",
    "                  'labels': y_train}\n",
    "\n",
    "## restructure test data\n",
    "all_test_data = {'features': a_test}\n",
    "\n",
    "# ### Cross-validation -- Tune Parameters\n",
    "X = all_train_data['features'].values.astype(np.float32)\n",
    "y = all_train_data['labels'].astype(np.int16)\n",
    "X_test = all_test_data['features'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.498828Z",
     "start_time": "2018-08-26T11:50:38.492197Z"
    }
   },
   "outputs": [],
   "source": [
    "# # NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1).values\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(features, tpot_data['target'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.807276Z",
     "start_time": "2018-08-26T11:50:38.504007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score on the training set was:-16.536910679310278\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=XGBRegressor(learning_rate=0.01, max_depth=1, min_child_weight=16, n_estimators=100, nthread=1, subsample=0.55)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.9, learning_rate=0.001, loss=\"ls\", max_depth=2, max_features=0.7500000000000001, min_samples_leaf=12, min_samples_split=17, n_estimators=100, subsample=1.0)),\n",
    "    Nystroem(gamma=0.25, kernel=\"laplacian\", n_components=10),\n",
    "    GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=\"lad\", max_depth=2, max_features=0.7000000000000001, min_samples_leaf=19, min_samples_split=7, n_estimators=100, subsample=0.6500000000000001)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X, y)\n",
    "results = exported_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T11:50:38.836663Z",
     "start_time": "2018-08-26T11:50:38.811271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am awesome.\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('../data/submission_format.csv')\n",
    "submission.loc[:, 'total_cases'] = np.array(results, dtype=np.int32)\n",
    "\n",
    "## Submission Format\n",
    "submission.to_csv('../data/submission_30_gen.csv', index=False)\n",
    "\n",
    "# no parameters unless we have a read_csv kwargs file\n",
    "v = DrivenDataValidator()\n",
    "\n",
    "if v.is_valid('../data/submission_format.csv', '../data/submission_30_gen.csv'):\n",
    "    print \"I am awesome.\"\n",
    "else:\n",
    "    print \"I am not so cool.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
