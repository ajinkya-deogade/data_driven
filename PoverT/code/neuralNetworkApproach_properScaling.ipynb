{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import interp\n",
    "from drivendata_validator import DrivenDataValidator\n",
    "import itertools\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('..', 'data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Submission DataFrame\n",
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds[:, 1],  # proba p=1\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Household-level survey data: \n",
    "This is obfuscated data from surveys conducted by The World Bank, focusing on household-level statistics. The data come from three different countries, and are separated into different files for convenience.\n",
    "\n",
    "##### Individual-level survey data: \n",
    "This is obfuscated data from related surveys conducted by The World Bank, only these focus on individual-level statistics. The set of interviewees and countries involved are the same as the household data, as indicated by shared id indices, but this data includes detailed (obfuscated) information about household members.\n",
    "\n",
    "##### Submission format:\n",
    "This gives us the filenames and columns of our submission prediction, filled with all 0.5 as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define data paths\n",
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A', 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A', 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B', 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B', 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C', 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C', 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first thing to notice is that each country's surveys have wildly different numbers of columns, so we'll plan on training separate models for each country and combining our predictions for submission at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Training\n",
      "Country A\n",
      "Shapes before transformation\n",
      "('Train : ', (8203, 859))\n",
      "('Test : ', (4041, 859))\n",
      "('Train + Test : ', (12244, 859))\n",
      "Shapes after transformation\n",
      "('Train : ', (8203, 860))\n",
      "('Test : ', (4041, 859))\n",
      "\n",
      "Country B\n",
      "Shapes before transformation\n",
      "('Train : ', (3255, 1432))\n",
      "('Test : ', (1604, 1432))\n",
      "('Train + Test : ', (4859, 1432))\n",
      "Shapes after transformation\n",
      "('Train : ', (3255, 1433))\n",
      "('Test : ', (1604, 1432))\n",
      "\n",
      "Country C\n",
      "Shapes before transformation\n",
      "('Train : ', (6469, 795))\n",
      "('Test : ', (3187, 795))\n",
      "('Train + Test : ', (9656, 795))\n",
      "Shapes after transformation\n",
      "('Train : ', (6469, 796))\n",
      "('Test : ', (3187, 795))\n"
     ]
    }
   ],
   "source": [
    "def pre_process_train_test_data(train, test):\n",
    "    labels = np.ravel(train.poor)\n",
    "    train = pd.get_dummies(train.drop('poor', axis=1))\n",
    "    test =  pd.get_dummies(test)\n",
    "\n",
    "    # match test set and training set columns\n",
    "    to_drop = np.setdiff1d(test.columns, train.columns)\n",
    "    to_add = np.setdiff1d(train.columns, test.columns)\n",
    "\n",
    "    test.drop(to_drop, axis=1, inplace=True)\n",
    "    test = test.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    train.fillna(0, inplace=True)\n",
    "    test.fillna(0, inplace=True)\n",
    "\n",
    "    test_indices = test.index\n",
    "    train_indices = train.index\n",
    "    train_test = pd.concat([train, test])\n",
    "\n",
    "    print(\"Shapes before transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \", test.shape)\n",
    "    print(\"Train + Test : \", train_test.shape)\n",
    "\n",
    "    numeric_vals = train_test.select_dtypes(include=['int64', 'float64'])\n",
    "    scaler = StandardScaler()\n",
    "    train_test[numeric_vals.columns] = scaler.fit_transform(numeric_vals)\n",
    "\n",
    "    train = train_test.loc[train_indices, :]\n",
    "    test  = train_test.loc[test_indices, :]\n",
    "    \n",
    "    train['poor'] = labels\n",
    "    \n",
    "    print(\"Shapes after transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \",  test.shape)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "print(\"Preprocessing Training\")\n",
    "print(\"Country A\")\n",
    "a_train, a_test = pre_process_train_test_data(a_train, a_test)\n",
    "aX_train = a_train.drop('poor', axis=1)\n",
    "ay_train = np.ravel(a_train.poor)\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train, b_test = pre_process_train_test_data(b_train, b_test)\n",
    "bX_train = b_train.drop('poor', axis=1)\n",
    "by_train = np.ravel(b_train.poor)\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train, c_test = pre_process_train_test_data(c_train, c_test)\n",
    "cX_train = c_train.drop('poor', axis=1)\n",
    "cy_train = np.ravel(c_train.poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restructure train data\n",
    "all_train_data = {'A': {'features': aX_train, \n",
    "                    'labels': ay_train},\n",
    "                  'B': {'features': bX_train,\n",
    "                        'labels':  by_train}, \n",
    "                  'C': {'features': cX_train, \n",
    "                        'labels':  cy_train}}\n",
    "\n",
    "## restructure test data\n",
    "all_test_data = {'A': {'features': a_test},\n",
    "                 'B': {'features': b_test},\n",
    "                 'C': {'features': c_test}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation -- Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the previously optimized parameters......\n"
     ]
    }
   ],
   "source": [
    "tune_params = 0\n",
    "if tune_params > 0:\n",
    "    bestParams = []\n",
    "    cv = StratifiedShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n",
    "    for grp in all_train_data:\n",
    "\n",
    "        # get some data\n",
    "        X = all_train_data[grp]['features'].values.astype(np.float32)\n",
    "        y = all_train_data[grp]['labels'].astype(np.int16)\n",
    "\n",
    "        # build a classifier\n",
    "        # sys.stdout = open(\"mlp_param_tuning_%s.txt\"%(grp), \"w\")\n",
    "        clf = MLPClassifier(learning_rate='adaptive', solver='lbfgs', \n",
    "                            verbose=10, tol=1e-4, random_state=1, \n",
    "                            learning_rate_init=.1)\n",
    "\n",
    "        param_dist = {'hidden_layer_sizes': \n",
    "                      [x for x in itertools.product((32, 64, 128, 256), repeat=1)] + \n",
    "                      [x for x in itertools.product((32, 64, 128, 256), repeat=2)] + \n",
    "                      [x for x in itertools.product((32, 64, 128, 256), repeat=3)],\n",
    "                      'alpha': [0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "    \n",
    "        # run randomized search\n",
    "        n_iter_search = 1000\n",
    "        random_search = RandomizedSearchCV(clf, scoring='neg_log_loss', \n",
    "                                           param_distributions=param_dist, \n",
    "                                           n_iter=n_iter_search, cv=cv,\n",
    "                                           return_train_score=False, n_jobs=16)\n",
    "\n",
    "        start = time()\n",
    "        random_search.fit(X, y)\n",
    "        gridSearchScores = pd.DataFrame(random_search.cv_results_)\n",
    "        gridSearchScores.sort_values(['mean_test_score'], axis=0, ascending=False, inplace=True)\n",
    "        gridSearchScores.head()\n",
    "\n",
    "        all_train_data[grp]['best_parameters'] = gridSearchScores.iloc[0,:]\n",
    "        bestParams.append(all_train_data[grp]['best_parameters'])\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "    bestParamsDF = pd.DataFrame(bestParams, index=['A', 'B', 'C'])\n",
    "    bestParamsDF.to_csv('bestParametersAllThreeModels_nn_%s.txt'%(timestamp), sep='\\t', index_label='group')\n",
    "    print \"Best Parameters.....\\n\", bestParamsDF\n",
    "else:\n",
    "    print 'Using the previously optimized parameters......'\n",
    "    bestParamsDF = pd.read_csv('bestParametersAllThreeModels_nn_20180217_2112.txt', sep='\\t', index_col='group')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinkyadeogade/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:327: DeprecationWarning: unorderable dtypes; returning scalar but in the future this will be an error\n",
      "  if np.any(np.array(hidden_layer_sizes) <= 0):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31dee2729679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                         \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                         random_state=1, learning_rate_init='adaptive')\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ajinkyadeogade/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \"\"\"\n\u001b[1;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 973\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajinkyadeogade/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    347\u001b[0m                                            incremental):\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# First time training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# lbfgs does not support mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajinkyadeogade/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, y, layer_units)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             coef_init, intercept_init = self._init_coef(layer_units[i],\n\u001b[0;32m--> 287\u001b[0;31m                                                         layer_units[i + 1])\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajinkyadeogade/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_init_coef\u001b[0;34m(self, fan_in, fan_out)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0minit_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'identity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0minit_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m# this was caught earlier, just to make sure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "## Train Model\n",
    "trained_models = {}\n",
    "for grp in all_train_data:\n",
    "    trained_models[grp] = MLPClassifier(hidden_layer_sizes=bestParamsDF.loc[grp, 'param_hidden_layer_sizes'],\n",
    "                                        alpha=bestParamsDF.loc[grp, 'param_alpha'],\n",
    "                                        solver='lbfgs',verbose=10, tol=1e-4, \n",
    "                                        random_state=1, learning_rate_init='adaptive')\n",
    "    trained_models[grp].fit(all_train_data[grp]['features'], all_train_data[grp]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict\n",
    "predictions = {}\n",
    "for grp in all_train_data:\n",
    "    predictions[grp] = trained_models[grp].predict_proba(all_test_data[grp]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert preds to data frames\n",
    "predictionsDF = {}\n",
    "for grp in all_train_data:\n",
    "    predictionsDF[grp] = make_country_sub(predictions[grp], all_test_data[grp]['features'], grp)\n",
    "\n",
    "submission = []\n",
    "submission = pd.concat([predictionsDF['A'], predictionsDF['B'], predictionsDF['C']])\n",
    "\n",
    "## Submission Format\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "submission.to_csv('../data/%s_nn_submission.csv'%(timestamp))\n",
    "\n",
    "# no parameters unless we have a read_csv kwargs file\n",
    "v = DrivenDataValidator()\n",
    "\n",
    "if v.is_valid('../data/submission_format.csv', '../data/%s_nn_submission.csv'%(timestamp)):\n",
    "    print \"I am awesome.\"\n",
    "else:\n",
    "    print \"I am not so cool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>101.713089</td>\n",
       "      <td>0.341195</td>\n",
       "      <td>-0.612180</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>{'hidden_layer_sizes': (256,), 'alpha': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.535704</td>\n",
       "      <td>-0.662634</td>\n",
       "      <td>-0.623232</td>\n",
       "      <td>-0.627149</td>\n",
       "      <td>16.227904</td>\n",
       "      <td>0.156087</td>\n",
       "      <td>0.046745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>147.248334</td>\n",
       "      <td>0.406603</td>\n",
       "      <td>-0.326795</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 32, 64)</td>\n",
       "      <td>{'hidden_layer_sizes': (32, 32, 64), 'alpha': ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.321011</td>\n",
       "      <td>-0.302953</td>\n",
       "      <td>-0.307679</td>\n",
       "      <td>-0.375536</td>\n",
       "      <td>2.300468</td>\n",
       "      <td>0.097555</td>\n",
       "      <td>0.028909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>95.764336</td>\n",
       "      <td>0.359003</td>\n",
       "      <td>-0.864010</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>{'hidden_layer_sizes': (32,), 'alpha': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.885142</td>\n",
       "      <td>-0.989020</td>\n",
       "      <td>-0.830927</td>\n",
       "      <td>-0.750951</td>\n",
       "      <td>9.720902</td>\n",
       "      <td>0.109801</td>\n",
       "      <td>0.086532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_fit_time  mean_score_time  mean_test_score  param_alpha  \\\n",
       "group                                                                 \n",
       "A         101.713089         0.341195        -0.612180        0.100   \n",
       "B         147.248334         0.406603        -0.326795        0.001   \n",
       "C          95.764336         0.359003        -0.864010        0.100   \n",
       "\n",
       "      param_hidden_layer_sizes  \\\n",
       "group                            \n",
       "A                       (256,)   \n",
       "B                 (32, 32, 64)   \n",
       "C                        (32,)   \n",
       "\n",
       "                                                  params  rank_test_score  \\\n",
       "group                                                                       \n",
       "A           {'hidden_layer_sizes': (256,), 'alpha': 0.1}                1   \n",
       "B      {'hidden_layer_sizes': (32, 32, 64), 'alpha': ...                1   \n",
       "C            {'hidden_layer_sizes': (32,), 'alpha': 0.1}                1   \n",
       "\n",
       "       split0_test_score  split1_test_score  split2_test_score  \\\n",
       "group                                                            \n",
       "A              -0.535704          -0.662634          -0.623232   \n",
       "B              -0.321011          -0.302953          -0.307679   \n",
       "C              -0.885142          -0.989020          -0.830927   \n",
       "\n",
       "       split3_test_score  std_fit_time  std_score_time  std_test_score  \n",
       "group                                                                   \n",
       "A              -0.627149     16.227904        0.156087        0.046745  \n",
       "B              -0.375536      2.300468        0.097555        0.028909  \n",
       "C              -0.750951      9.720902        0.109801        0.086532  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestParamsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(32,)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestParamsDF.loc['C', 'param_hidden_layer_sizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
