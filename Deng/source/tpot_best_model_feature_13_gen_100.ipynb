{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:45:59.805868Z",
     "start_time": "2018-08-27T01:45:57.121813Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before transformation\n",
      "('Train : ', (1456, 23))\n",
      "('Train Labels : ', (1456, 4))\n",
      "('Test : ', (416, 23))\n",
      "('Columns : ', Index([u'city', u'year', u'weekofyear', u'ndvi_ne', u'ndvi_nw', u'ndvi_se',\n",
      "       u'ndvi_sw', u'precipitation_amt_mm', u'reanalysis_air_temp_k',\n",
      "       u'reanalysis_avg_temp_k', u'reanalysis_dew_point_temp_k',\n",
      "       u'reanalysis_max_air_temp_k', u'reanalysis_min_air_temp_k',\n",
      "       u'reanalysis_precip_amt_kg_per_m2',\n",
      "       u'reanalysis_relative_humidity_percent',\n",
      "       u'reanalysis_sat_precip_amt_mm',\n",
      "       u'reanalysis_specific_humidity_g_per_kg', u'reanalysis_tdtr_k',\n",
      "       u'station_avg_temp_c', u'station_diur_temp_rng_c',\n",
      "       u'station_max_temp_c', u'station_min_temp_c', u'station_precip_mm'],\n",
      "      dtype='object'))\n",
      "  city  year  weekofyear   ndvi_ne   ndvi_nw   ndvi_se   ndvi_sw  pred_precip  \\\n",
      "0   sj  1990          18  0.122600  0.103725  0.198483  0.177617        12.42   \n",
      "1   sj  1990          19  0.169900  0.142175  0.162357  0.155486        22.82   \n",
      "2   sj  1990          20  0.032250  0.172967  0.157200  0.170843        34.54   \n",
      "3   sj  1990          21  0.128633  0.245067  0.227557  0.235886        15.36   \n",
      "4   sj  1990          22  0.196200  0.262200  0.251200  0.247340         7.52   \n",
      "\n",
      "   pred_temp  pred_avg_temp     ...       pred_rel_humidity_per  \\\n",
      "0  24.422857      24.592857     ...                   73.365714   \n",
      "1  25.061429      25.292857     ...                   77.368571   \n",
      "2  25.631429      25.728571     ...                   82.052857   \n",
      "3  25.837143      26.078571     ...                   80.337143   \n",
      "4  26.368571      26.514286     ...                   80.460000   \n",
      "\n",
      "   pred_sat_precip  pred_spec_humidity  pred_temp_rng  station_avg_temp_c  \\\n",
      "0            12.42           14.012857       2.628571           25.442857   \n",
      "1            22.82           15.372857       2.371429           26.714286   \n",
      "2            34.54           16.848571       2.300000           26.714286   \n",
      "3            15.36           16.672857       2.428571           27.471429   \n",
      "4             7.52           17.210000       3.014286           28.942857   \n",
      "\n",
      "   station_diur_temp_rng_c  station_max_temp_c  station_min_temp_c  \\\n",
      "0                 6.900000                29.4                20.0   \n",
      "1                 6.371429                31.7                22.2   \n",
      "2                 6.485714                32.2                22.8   \n",
      "3                 6.771429                33.3                23.3   \n",
      "4                 9.371429                35.0                23.9   \n",
      "\n",
      "   station_precip_mm  total_cases  \n",
      "0               16.0            4  \n",
      "1                8.6            5  \n",
      "2               41.4            4  \n",
      "3                4.0            3  \n",
      "4                5.8            6  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Preprocessing Training\n",
      "Shapes before transformation\n",
      "('Train : ', (1456, 13))\n",
      "('Test : ', (416, 13))\n",
      "('Train + Test : ', (1872, 13))\n",
      "Shapes after transformation\n",
      "('Train : ', (1456, 14))\n",
      "('Test : ', (416, 13))\n",
      "I am awesome.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import interp\n",
    "from drivendata_validator import DrivenDataValidator\n",
    "import itertools\n",
    "from tpot import TPOTRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def pre_process_train_test_data(train, test, label_var, exclude_scaling):\n",
    "    labels = np.ravel(train[label_var])\n",
    "    train = pd.get_dummies(train.drop(label_var, axis=1))\n",
    "    test = pd.get_dummies(test)\n",
    "\n",
    "    # match test set and training set columns\n",
    "    to_drop = np.setdiff1d(test.columns, train.columns)\n",
    "    to_add = np.setdiff1d(train.columns, test.columns)\n",
    "\n",
    "    test.drop(to_drop, axis=1, inplace=True)\n",
    "    test = test.assign(**{c: 0 for c in to_add})\n",
    "\n",
    "    test_indices = test.index\n",
    "    train_indices = train.index\n",
    "    train_test = pd.concat([train, test])\n",
    "    # train_test.sort_values(['year', 'weekofyear'], inplace=True)\n",
    "    train_test.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    print(\"Shapes before transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \", test.shape)\n",
    "    print(\"Train + Test : \", train_test.shape)\n",
    "\n",
    "    numeric_vals = train_test.select_dtypes(include=['int64', 'float64'])\n",
    "    numeric_vals = numeric_vals.loc[:, [x for x in list(\n",
    "        numeric_vals.columns.values) if x not in exclude_scaling]]\n",
    "    scaler = StandardScaler()\n",
    "    train_test[numeric_vals.columns] = scaler.fit_transform(numeric_vals)\n",
    "\n",
    "    train = train_test.loc[train_indices, :]\n",
    "    test = train_test.loc[test_indices, :]\n",
    "\n",
    "    train[label_var] = labels\n",
    "\n",
    "    print(\"Shapes after transformation\")\n",
    "    print(\"Train : \", train.shape)\n",
    "    print(\"Test : \",  test.shape)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "# DATA_DIR = 'D:\\work\\git_repos\\data_driven\\Deng\\data'\n",
    "\n",
    "# define data paths\n",
    "data_paths = {'train_x': os.path.join(DATA_DIR, 'dengue_features_train.csv'),\n",
    "              'train_y': os.path.join(DATA_DIR, 'dengue_labels_train.csv'),\n",
    "              'test_x':  os.path.join(DATA_DIR, 'dengue_features_test.csv')}\n",
    "\n",
    "# load training data\n",
    "X_train = pd.read_csv(data_paths['train_x'])\n",
    "y_train = pd.read_csv(data_paths['train_y'])\n",
    "X_train.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# load test data\n",
    "X_test = pd.read_csv(data_paths['test_x'])\n",
    "X_test.drop(columns='week_start_date', inplace=True)\n",
    "\n",
    "# #### The first thing to notice is that each country's surveys have wildly different numbers of columns, so we'll plan on training separate models for each country and combining our predictions for submission at the end.\n",
    "# ### Pre-process Data\n",
    "print(\"Shapes before transformation\")\n",
    "print(\"Train : \", X_train.shape)\n",
    "print(\"Train Labels : \", y_train.shape)\n",
    "print(\"Test : \", X_test.shape)\n",
    "print(\"Columns : \", X_train.columns)\n",
    "train_data = pd.merge(X_train, y_train, on=['city', 'year', 'weekofyear'])\n",
    "train_data.index = np.arange(0, train_data.shape[0])\n",
    "X_test.index = np.arange(\n",
    "    train_data.shape[0]+1, train_data.shape[0]+X_test.shape[0]+1)\n",
    "\n",
    "# Rename Variables\n",
    "train_data = train_data.rename(columns={'precipitation_amt_mm': 'pred_precip',\n",
    "                                        'reanalysis_air_temp_k': 'pred_temp',\n",
    "                                        'reanalysis_avg_temp_k': 'pred_avg_temp',\n",
    "                                        'reanalysis_dew_point_temp_k': 'pred_dew_temp',\n",
    "                                        'reanalysis_max_air_temp_k': 'pred_max_temp',\n",
    "                                        'reanalysis_min_air_temp_k': 'pred_min_temp',\n",
    "                                        'reanalysis_precip_amt_kg_per_m2': 'pred_precip_vol',\n",
    "                                        'reanalysis_specific_humidity_g_per_kg': 'pred_spec_humidity',\n",
    "                                        'reanalysis_tdtr_k': 'pred_temp_rng',\n",
    "                                        'reanalysis_relative_humidity_percent': 'pred_rel_humidity_per',\n",
    "                                        'reanalysis_sat_precip_amt_mm': 'pred_sat_precip'\n",
    "                                        })\n",
    "train_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "X_test = X_test.rename(columns={'precipitation_amt_mm': 'pred_precip',\n",
    "                                'reanalysis_air_temp_k': 'pred_temp',\n",
    "                                'reanalysis_avg_temp_k': 'pred_avg_temp',\n",
    "                                'reanalysis_dew_point_temp_k': 'pred_dew_temp',\n",
    "                                'reanalysis_max_air_temp_k': 'pred_max_temp',\n",
    "                                'reanalysis_min_air_temp_k': 'pred_min_temp',\n",
    "                                'reanalysis_precip_amt_kg_per_m2': 'pred_precip_vol',\n",
    "                                'reanalysis_specific_humidity_g_per_kg': 'pred_spec_humidity',\n",
    "                                'reanalysis_tdtr_k': 'pred_temp_rng',\n",
    "                                'reanalysis_relative_humidity_percent': 'pred_rel_humidity_per',\n",
    "                                'reanalysis_sat_precip_amt_mm': 'pred_sat_precip'\n",
    "                                })\n",
    "X_test.interpolate(method='linear', inplace=True)\n",
    "\n",
    "kelvin_cols = ['pred_temp', 'pred_avg_temp',\n",
    "               'pred_dew_temp', 'pred_max_temp', 'pred_min_temp']\n",
    "train_data.loc[:, kelvin_cols] = train_data.loc[:, kelvin_cols].copy() - 273.15\n",
    "X_test.loc[:, kelvin_cols] = X_test.loc[:, kelvin_cols].copy() - 273.15\n",
    "print(train_data.head())\n",
    "\n",
    "pred_cols = ['pred_precip', 'pred_temp', 'pred_avg_temp',\n",
    "             'pred_dew_temp', 'pred_max_temp', 'pred_min_temp',\n",
    "             'pred_temp_rng', 'pred_precip_vol', 'pred_rel_humidity_per',\n",
    "             'pred_sat_precip', 'pred_spec_humidity']\n",
    "\n",
    "sta_cols = ['station_avg_temp_c', 'station_min_temp_c', 'station_max_temp_c',\n",
    "            'station_diur_temp_rng_c', 'station_precip_mm']\n",
    "\n",
    "veg_cols = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "\n",
    "loc_cols = ['city', 'year', 'weekofyear']\n",
    "\n",
    "# to_drop = ['pred_precip', 'pred_temp', 'pred_avg_temp',\n",
    "#           'pred_dew_temp', 'pred_max_temp', 'pred_min_temp',\n",
    "#           'pred_temp_rng', 'pred_precip_vol', 'pred_rel_humidity_per',\n",
    "#           'pred_sat_precip', 'year', 'weekofyear', 'ndvi_se', 'ndvi_nw']\n",
    "\n",
    "to_drop = ['pred_sat_precip', 'pred_dew_temp', 'pred_temp_rng',\n",
    "           'pred_avg_temp', 'station_avg_temp_c', 'station_diur_temp_rng_c',\n",
    "           'ndvi_se', 'ndvi_nw', 'pred_max_temp', 'pred_min_temp', 'pred_temp']\n",
    "\n",
    "# to_drop = ['pred_sat_precip', 'pred_dew_temp', 'pred_temp_rng',\n",
    "#            'pred_avg_temp', 'station_avg_temp_c', 'station_diur_temp_rng_c',\n",
    "#            'ndvi_se', 'ndvi_nw', 'pred_max_temp', 'pred_min_temp', 'pred_temp', 'year']\n",
    "\n",
    "train_data_drop = train_data.drop(to_drop, axis=1)\n",
    "X_test_drop = X_test.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"Preprocessing Training\")\n",
    "label_var = 'total_cases'\n",
    "exclude_scaling = []\n",
    "a_train, a_test = pre_process_train_test_data(\n",
    "    train_data_drop, X_test_drop, label_var, exclude_scaling)\n",
    "X_train = a_train.drop(label_var, axis=1)\n",
    "y_train = np.ravel(a_train[label_var])\n",
    "\n",
    "# restructure train data\n",
    "all_train_data = {'features': X_train,\n",
    "                  'labels': y_train}\n",
    "\n",
    "# restructure test data\n",
    "all_test_data = {'features': a_test}\n",
    "\n",
    "tune_params = 0\n",
    "if tune_params > 0:\n",
    "    bestParams = []\n",
    "    X = all_train_data['features'].values.astype(np.float32)\n",
    "    y = all_train_data['labels'].astype(np.int32)\n",
    "    mae_score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    #         pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=5,\n",
    "    #                                             periodic_checkpoint_folder='tpot_best_models_100',\n",
    "    #                                             n_jobs=20, random_state=42, verbosity=1, memory='auto',\n",
    "    #                                             generations=100, max_eval_time_mins=10)\n",
    "    #         pipeline_optimizer.fit(X, y)\n",
    "    #         pipeline_optimizer.export('tpot_best_model_pipeline_gen_100_mae.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring='neg_mean_absolute_error', cv=5,\n",
    "    #                            periodic_checkpoint_folder='D:\\work\\git_repos\\data_driven\\Deng\\data\\tpot_best_models_500\\',\n",
    "    #                            n_jobs=20, random_state=42, verbosity=3, memory='auto',\n",
    "    #                            generations=500, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('D:\\work\\git_repos\\data_driven\\Deng\\source\\tpot_best_model_pipeline_gen500.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                        periodic_checkpoint_folder='tpot_best_models_100_feat_13',\n",
    "    #                        n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                        generations=100, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('tpot_best_model_feature_13_gen_100.py')\n",
    "\n",
    "    #     pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                     periodic_checkpoint_folder='tpot_best_models_300_feat_12',\n",
    "    #                     n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                     generations=500, max_eval_time_mins=10)\n",
    "    #     pipeline_optimizer.fit(X, y)\n",
    "    #     pipeline_optimizer.export('tpot_best_model_feature_12_gen_300.py')\n",
    "\n",
    "    # pipeline_optimizer = TPOTRegressor(scoring=mae_score, cv=10,\n",
    "    #                periodic_checkpoint_folder='tpot_best_models_100_feat_8',\n",
    "    #                n_jobs=20, random_state=42, verbosity=2, memory='auto',\n",
    "    #                generations=100, max_eval_time_mins=10)\n",
    "    # pipeline_optimizer.fit(X, y)\n",
    "    # pipeline_optimizer.export('tpot_best_model_feature_8_gen_100.py')\n",
    "\n",
    "get_predictions = 1\n",
    "if get_predictions > 0:\n",
    "    X = all_train_data['features'].values.astype(np.float32)\n",
    "    y = all_train_data['labels'].astype(np.int16)\n",
    "    X_test = all_test_data['features'].values.astype(np.float32)\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.feature_selection import SelectFwe, SelectPercentile, f_regression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import make_pipeline, make_union\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from tpot.builtins import StackingEstimator\n",
    "    from xgboost import XGBRegressor\n",
    "    # Score on the training set was:-15.156138879002743\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.75, learning_rate=0.01, loss=\"quantile\", max_depth=1,\n",
    "                                                              max_features=0.35000000000000003, min_samples_leaf=10, min_samples_split=20, n_estimators=100, subsample=0.7500000000000001)),\n",
    "        SelectFwe(score_func=f_regression, alpha=0.027),\n",
    "        SelectPercentile(score_func=f_regression, percentile=72),\n",
    "        PolynomialFeatures(degree=2, include_bias=False,\n",
    "                           interaction_only=False),\n",
    "        XGBRegressor(learning_rate=0.01, max_depth=4, min_child_weight=14,\n",
    "                     n_estimators=100, nthread=1, subsample=0.15000000000000002)\n",
    "    )\n",
    "\n",
    "    exported_pipeline.fit(X, y)\n",
    "    results = exported_pipeline.predict(X_test)\n",
    "    \n",
    "    submission = pd.read_csv('../data/submission_format.csv')\n",
    "    submission.loc[:, 'total_cases'] = np.array(np.ceil(results), dtype=np.int32)\n",
    "\n",
    "    ## Submission Format\n",
    "    submission.to_csv('../data/submission_feature_13_gen_100.csv', index=False)\n",
    "\n",
    "    # no parameters unless we have a read_csv kwargs file\n",
    "    v = DrivenDataValidator()\n",
    "\n",
    "    if v.is_valid('../data/submission_format.csv', '../data/submission_feature_13_gen_100.csv'):\n",
    "        print \"I am awesome.\"\n",
    "    else:\n",
    "        print \"I am not so cool.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
